import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from openai import OpenAI
import gradio as gr
import os
from google.colab import userdata
URL = "https://drive.google.com/uc?export=download&id=13DSc38TKZoiO2C9Pby02gn5-DVQT6gX9"

# !wget -O faiss_db.zip "$URL"
# !unzip faiss_db.zip

class CustomE5Embedding(HuggingFaceEmbeddings):
    def embed_documents(self, texts):
        texts = [f"passage: {t}" for t in texts]
        return super().embed_documents(texts)

    def embed_query(self, text):
        return super().embed_query(f"query: {text}")

embedding_model = CustomE5Embedding(model_name="intfloat/multilingual-e5-small")
db = FAISS.load_local("faiss_db", embedding_model, allow_dangerous_deserialization=True)
retriever = db.as_retriever()
api_key = userdata.get('Groq') # api_key = your Groq api key
os.environ["OPENAI_API_KEY"] = api_key
model = "llama3-70b-8192"
base_url="https://api.groq.com/openai/v1"
client = OpenAI(
    base_url=base_url # ä½¿ç”¨ OpenAI æœ¬èº«ä¸éœ€è¦é€™æ®µ
)
system_prompt = "ä½ æ˜¯ä¸€ä½è‡ºç£çš„è‹±é›„è¯ç›Ÿçš„è§’è‰²å°ˆå®¶ï¼Œç†Ÿæ‚‰æ‰€æœ‰è‹±é›„çš„æŠ€èƒ½ã€ç‰¹æ€§å’Œç©æ³•ã€‚è«‹æ ¹æ“šæä¾›çš„è³‡æ–™ä¾†å›æ‡‰ç©å®¶çš„å•é¡Œã€‚å›ç­”è¦ç”Ÿå‹•æœ‰è¶£ï¼Œå¸¶æœ‰è‹±é›„è¯ç›Ÿçš„é¢¨æ ¼ï¼ŒåŒæ™‚æä¾›å¯¦ç”¨çš„éŠæˆ²å»ºè­°ã€‚è«‹ç”¨å°ç£åœ°å€å¸¸è¦‹çš„è‹±é›„è¯ç›Ÿè¡“èªå›æ‡‰ã€‚ä½¿ç”¨[ç¹é«”ä¸­æ–‡]å›ç­”ã€‚"

prompt_template = """
æ ¹æ“šä¸‹åˆ—è‹±é›„è³‡æ–™å›ç­”å•é¡Œï¼š
{retrieved_chunks}

å¬å–šå¸«çš„å•é¡Œæ˜¯ï¼š{question}

è«‹æ ¹æ“šè³‡æ–™å…§å®¹å›è¦†ï¼Œè‹¥è³‡æ–™ä¸­æ²’æœ‰è¶³å¤ ä¿¡æ¯ï¼Œè«‹å‘Šè¨´å¬å–šå¸«å¯ä»¥æŸ¥é–±è‹±é›„è¯ç›Ÿå®˜æ–¹ç¶²ç«™æˆ–è«®è©¢é«˜ç«¯ç©å®¶ç²å–æ›´å¤šå»ºè­°ã€‚å›ç­”æ™‚å¯é©ç•¶èå…¥è‹±é›„çš„ç¶“å…¸å°è©æˆ–éŠæˆ²ä¸­çš„å°ˆæ¥­è¡“èªã€‚ä½¿ç”¨"ç¹é«”ä¸­æ–‡"å›ç­”ã€‚
"""
chat_history = []

def chat_with_rag(user_input):
    global chat_history
    # å–å›ç›¸é—œè³‡æ–™
    docs = retriever.get_relevant_documents(user_input)
    retrieved_chunks = "\n\n".join([doc.page_content for doc in docs])

    # å°‡è‡ªå®š prompt å¥—å…¥æ ¼å¼
    final_prompt = prompt_template.format(retrieved_chunks=retrieved_chunks, question=user_input)

    # å‘¼å« OpenAI API
    response = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": final_prompt},
    ]
    )
    answer = response.choices[0].message.content

    chat_history.append((user_input, answer))
    return answer

with gr.Blocks() as demo:
    gr.Markdown("# ğŸ“ è‹±é›„è¯ç›Ÿçš„è§’è‰²å°ˆå®¶")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="è«‹è¼¸å…¥ä½ çš„å•é¡Œ...")

    def respond(message, chat_history_local):
        response = chat_with_rag(message)
        chat_history_local.append((message, response))
        return "", chat_history_local

    msg.submit(respond, [msg, chatbot], [msg, chatbot])

demo.launch(debug=True)
